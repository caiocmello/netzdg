{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiocmello/netzdg/blob/main/NetzDG_URLs_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **NetzDG URL Analysis (version 2)**\n",
        "This version includes all the URLs shared by the clusters. The dataset to be used is called 'final_dataframe_urls.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4XiQ11EujYku"
      },
      "outputs": [],
      "source": [
        "# @title Run to install packages (use csv file 'df_urls_inclRT2.csv')\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as en_stopwords\n",
        "from spacy.lang.de.stop_words import STOP_WORDS as de_stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import ngrams\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "df = pd.read_csv('final_dataframe_urls.csv')\n",
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "df['original_url'] = df['original_url'].astype(str)\n",
        "def clean_url(url):\n",
        "  parts = url.split('/')\n",
        "  if len(parts) >= 4:\n",
        "    return '/'.join(parts[:3])\n",
        "  else:\n",
        "    return url\n",
        "\n",
        "df['cleaned_url'] = df['original_url'].apply(clean_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QorDkf2jb2A"
      },
      "outputs": [],
      "source": [
        "# Run this cell to see the top 20 most tweeted URLs (change number in code to see more than 20)\n",
        "urls = df['original_url'].value_counts()\n",
        "print(urls[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3SdvSmDjfOh"
      },
      "outputs": [],
      "source": [
        "# @title See list of URLs posted by user\n",
        "user = \"hwieduwilt\" # @param {\"type\":\"string\"}\n",
        "user_df = df[df['user_username'] == user]\n",
        "url_counts = user_df['original_url'].value_counts()\n",
        "print(url_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30eOPpxujiID"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster BLUE\n",
        "\n",
        "df_blue = df[df['modularity_class'] == 'blue']\n",
        "url_countsb = df_blue['original_url'].value_counts()\n",
        "print(url_countsb[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suZdPcQsjjvt"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster PURPLE\n",
        "\n",
        "df_purple = df[df['modularity_class'] == 'purple']\n",
        "url_countsp = df_purple['original_url'].value_counts()\n",
        "print(url_countsp[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm_RaPMwjlPQ"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster ORANGE\n",
        "\n",
        "df_orange = df[df['modularity_class'] == 'orange']\n",
        "url_countso = df_orange['original_url'].value_counts()\n",
        "print(url_countso[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRx-lgbkjm8_"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster RED\n",
        "\n",
        "df_red = df[df['modularity_class'] == 'red']\n",
        "url_countsr = df_red['original_url'].value_counts()\n",
        "print(url_countsr[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL37vxiTjrQw"
      },
      "source": [
        "### Analysis of domains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w80tuwvvjq84"
      },
      "outputs": [],
      "source": [
        "# Run this cell to see the top 20 most tweeted DOMAINS (change number in code to see more than 20)\n",
        "urls_dom = df['cleaned_url'].value_counts()\n",
        "print(urls_dom[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vMCbd-vzju6n"
      },
      "outputs": [],
      "source": [
        "# @title See list of DOMAINS posted by user\n",
        "user2 = \"hwieduwilt\" # @param {\"type\":\"string\"}\n",
        "user_df2 = df[df['user_username'] == user2]\n",
        "url_counts_dom = user_df2['cleaned_url'].value_counts()\n",
        "print(url_counts_dom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FNvdgIyjw3H"
      },
      "outputs": [],
      "source": [
        "# Most shared DOMAINS in cluster BLUE\n",
        "\n",
        "url_countsbl = df_blue['cleaned_url'].value_counts()\n",
        "print(url_countsbl[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GCTpbgkjyU3"
      },
      "outputs": [],
      "source": [
        "# Most shared DOMAINS in cluster PURPLE\n",
        "\n",
        "url_countspu = df_purple['cleaned_url'].value_counts()\n",
        "print(url_countspu[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT2QZ2FRjzuF"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster ORANGE\n",
        "\n",
        "url_countsor = df_orange['cleaned_url'].value_counts()\n",
        "print(url_countsor[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ-yVjY3j1gj"
      },
      "outputs": [],
      "source": [
        "# Most shared URLs in cluster RED\n",
        "\n",
        "url_countsre = df_red['cleaned_url'].value_counts()\n",
        "print(url_countsre[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9alZTsBGt4ut"
      },
      "source": [
        "# Data visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OEi4ZHrR1IB1"
      },
      "outputs": [],
      "source": [
        "# @title See wordcloud with top 50 domains (including Twitter)\n",
        "# generate a wordcloud with values in column domain\n",
        "\n",
        "df['domain'] = df['cleaned_url'].str.replace('https://', '')\n",
        "df['domain'] = df['domain'].str.replace('www.', '')\n",
        "df['domain'] = df['domain'].str.replace('.com', '')\n",
        "df['domain'] = df['domain'].str.replace('http://', '')\n",
        "\n",
        "def remove_after_dot(domain):\n",
        "  if isinstance(domain, str):\n",
        "    parts = domain.split('.')\n",
        "    if len(parts) > 0:\n",
        "      return parts[0]\n",
        "  return domain\n",
        "\n",
        "df['domain'] = df['domain'].apply(remove_after_dot)\n",
        "counting = df['domain'].value_counts()\n",
        "\n",
        "df_wc = counting[:50]\n",
        "df_wc = df_wc.reset_index()\n",
        "\n",
        "text = \" \".join(df_wc['domain'].astype(str))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JP4WxvHN1V6U"
      },
      "outputs": [],
      "source": [
        "# @title See wordcloud with top 50 domains (without Twitter)\n",
        "# generate a wordcloud with values in column domain\n",
        "\n",
        "df['domain'] = df['cleaned_url'].str.replace('https://', '')\n",
        "df['domain'] = df['domain'].str.replace('www.', '')\n",
        "df['domain'] = df['domain'].str.replace('.com', '')\n",
        "df['domain'] = df['domain'].str.replace('http://', '')\n",
        "\n",
        "\n",
        "def remove_after_dot(domain):\n",
        "  if isinstance(domain, str):\n",
        "    parts = domain.split('.')\n",
        "    if len(parts) > 0:\n",
        "      return parts[0]\n",
        "  return domain\n",
        "\n",
        "df['domain'] = df['domain'].apply(remove_after_dot)\n",
        "counting = df['domain'].value_counts()\n",
        "\n",
        "df_wc = counting[:50]\n",
        "df_wc = df_wc.reset_index()\n",
        "df_wc = df_wc[df_wc['domain'] != 'twitter']\n",
        "\n",
        "text = \" \".join(df_wc['domain'].astype(str))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5wKtNjwjMAe"
      },
      "source": [
        "# See specific websites linked in tweets\n",
        "(Eg. https://www.youtube.com or https://www.facebook.com or https://www.tichyseinblick.de)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0s_2V3B0jl7S"
      },
      "outputs": [],
      "source": [
        "# @title Run to see list of tweets mentioning website\n",
        "website = \"https://www.youtube.com\" # @param {\"type\":\"string\"}\n",
        "selected_rows = df[df['cleaned_url'].isin([website])]\n",
        "selected_rows.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKiigfAcla-b"
      },
      "outputs": [],
      "source": [
        "# See how many times the website chosen above is mentioned in corpus (Found in 'Index: XXXX entries')\n",
        "selected_rows.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EHRJdzGrRgO"
      },
      "outputs": [],
      "source": [
        "# See how many times each cluster mentions website chosen above (PS: too see results for a different URL, run the website search again!)\n",
        "\n",
        "cluster_counts = selected_rows['modularity_class'].value_counts()\n",
        "print(cluster_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FFtfPzcmiC8"
      },
      "outputs": [],
      "source": [
        "# See the most mentioned links for specific website chosen above (PS: to see more websites, replace number '10' in code)\n",
        "url_counts2 = selected_rows['original_url'].value_counts()\n",
        "print(url_counts2[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k5ldL8gmzYx"
      },
      "outputs": [],
      "source": [
        "# Most mentioned URLs in cluster blue for specific website chosen above\n",
        "\n",
        "df_blue2 = selected_rows[selected_rows['modularity_class'] == 'blue']\n",
        "url_countsbl2 = df_blue2['original_url'].value_counts()\n",
        "print(url_countsbl2[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGg8CXDDn3m0"
      },
      "outputs": [],
      "source": [
        "# Most mentioned URLs in cluster purple for specific website chosen above\n",
        "\n",
        "df_purple2 = selected_rows[selected_rows['modularity_class'] == 'purple']\n",
        "url_countspu2 = df_purple2['original_url'].value_counts()\n",
        "print(url_countspu2[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evqmPiAhoQ62"
      },
      "outputs": [],
      "source": [
        "# Most mentioned URLs in cluster red for specific website chosen above\n",
        "\n",
        "df_red2 = selected_rows[selected_rows['modularity_class'] == 'red']\n",
        "url_countsre2 = df_red2['original_url'].value_counts()\n",
        "print(url_countsre2[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxlacNevprZu"
      },
      "outputs": [],
      "source": [
        "# Most mentioned URLs in cluster orange for specific website chosen above\n",
        "\n",
        "df_orange2 = selected_rows[selected_rows['modularity_class'] == 'orange']\n",
        "url_countsor2 = df_orange2['original_url'].value_counts()\n",
        "print(url_countsor2[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MJ_HdEqfp_gU"
      },
      "outputs": [],
      "source": [
        "# @title Run to see list of n-grams (for texts in tweets mentioning the URL chosen above) - Entire dataset\n",
        "\n",
        "df_NetzClean = selected_rows.copy()\n",
        "\n",
        "# Remove underscore from tweets (To avoid erros in deleting users like @balzer_sascha)\n",
        "\n",
        "def cleaner(text):\n",
        "    text = re.sub(r\"_\", \"\", text) # Remove underscore\n",
        "    return text\n",
        "\n",
        "df_NetzClean['text_clean'] = df_NetzClean['text'].map(lambda x: cleaner(x))\n",
        "\n",
        "# Remove users, remove URLs, remove hashtag sign\n",
        "\n",
        "def cleaner(text):\n",
        "    text = re.sub(\"@[A-Za-z0-9]+\",\"\",text) # Remove @ sign\n",
        "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text) # Remove http links\n",
        "    text = \" \".join(text.split())\n",
        "    text = text.replace(\"#\", \"\") # Remove hashtag sign but keep the text\n",
        "    return text\n",
        "\n",
        "df_NetzClean['text_clean'] = df_NetzClean['text'].map(lambda x: cleaner(x))\n",
        "df_NetzClean = df_NetzClean.drop_duplicates(subset=['text']) #remove duplicated tweets in column 'text'\n",
        "\n",
        "ngrams = 3 # @param {type:\"raw\"}\n",
        "\n",
        "def get_top_n_words(corpus, stopwords, n=20):\n",
        "    vec = CountVectorizer(stop_words = stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def get_top_n_bigram(corpus, stopwords, n=20):\n",
        "    vec = CountVectorizer(ngram_range=(ngrams, ngrams), stop_words = stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def get_top_n_words_tfidf(corpus, stopwords, n=20):\n",
        "    vec = TfidfVectorizer(stop_words = stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def get_top_n_bigram_tfidf(corpus, stopwords, n=20):\n",
        "    vec = TfidfVectorizer(ngram_range=(2, 2), stop_words = stopwords).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Transform stopwords in a list\n",
        "\n",
        "stopwords_de = list(de_stopwords)\n",
        "stopwords_en = list(en_stopwords)\n",
        "\n",
        "stopwords = stopwords_de + stopwords_en #create a mixed list of stopwords (German and English)\n",
        "\n",
        "# Edit your list of stopwords manually\n",
        "\n",
        "add_to_stopwords = ['rt']\n",
        "stopwords = stopwords + add_to_stopwords\n",
        "\n",
        "content = df_NetzClean[['text_clean']]\n",
        "content.reset_index(drop=True, inplace=True)\n",
        "content = content.values.tolist()\n",
        "\n",
        "string = '\\n'.join(str(e) for e in content)\n",
        "\n",
        "items_in_list = 10 # @param {type:\"number\"}\n",
        "\n",
        "unigrams = get_top_n_words([string], stopwords=stopwords, n=items_in_list)\n",
        "bigrams = get_top_n_bigram([string], stopwords=stopwords, n=items_in_list)\n",
        "see_top_words = bigrams\n",
        "\n",
        "see_top_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnuGPK23tVhZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOqnJgQ1UAl8GszjSvEE8bH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
