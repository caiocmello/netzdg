{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fcb050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trafilatura\n",
      "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from trafilatura) (2024.12.14)\n",
      "Collecting charset_normalizer>=3.4.0 (from trafilatura)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting courlan>=1.3.2 (from trafilatura)\n",
      "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting htmldate>=1.9.2 (from trafilatura)\n",
      "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.1 (from trafilatura)\n",
      "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting lxml>=5.3.0 (from trafilatura)\n",
      "  Downloading lxml-6.0.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from trafilatura) (2.2.3)\n",
      "Collecting babel>=2.16.0 (from courlan>=1.3.2->trafilatura)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
      "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
      "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting lxml>=5.3.0 (from trafilatura)\n",
      "  Downloading lxml-5.4.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /opt/anaconda3/lib/python3.12/site-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
      "Collecting pytz>=2024.2 (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /opt/anaconda3/lib/python3.12/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.9.11)\n",
      "Collecting tzlocal>=0.2 (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.16.0)\n",
      "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-macosx_10_13_universal2.whl (199 kB)\n",
      "Downloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
      "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
      "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.4.0-cp312-cp312-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
      "Downloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytz, tzlocal, tld, lxml, charset_normalizer, babel, lxml_html_clean, dateparser, courlan, htmldate, justext, trafilatura\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.2.1\n",
      "    Uninstalling lxml-5.2.1:\n",
      "      Successfully uninstalled lxml-5.2.1\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: babel\n",
      "    Found existing installation: Babel 2.11.0\n",
      "    Uninstalling Babel-2.11.0:\n",
      "      Successfully uninstalled Babel-2.11.0\n",
      "Successfully installed babel-2.17.0 charset_normalizer-3.4.2 courlan-1.3.2 dateparser-1.2.2 htmldate-1.9.3 justext-3.0.2 lxml-5.4.0 lxml_html_clean-0.4.2 pytz-2025.2 tld-0.13.1 trafilatura-2.0.0 tzlocal-5.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9979053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trafilatura import fetch_url, extract\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ea45f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = pd.read_csv('/Users/caio.mello/Documents/NetzDG_project/NetzDG_data/philophia_perennis_scraping/philosophia_perennis_Gesetz+zur+Verbesserung+der+Rechtsdurchsetzung+in+sozialen+Netzwerken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7ff3596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://philosophia-perennis.com/2021/06/15/ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://philosophia-perennis.com/2018/03/05/rt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://philosophia-perennis.com/2017/06/29/bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                URL\n",
       "0           0  https://philosophia-perennis.com/2021/06/15/ze...\n",
       "1           1  https://philosophia-perennis.com/2018/03/05/rt...\n",
       "2           2  https://philosophia-perennis.com/2017/06/29/bu..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b70191b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_list = []\n",
    "processed_urls = []\n",
    "processed_urls_set = set(processed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "445b3867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://philosophia-perennis.com/2021/06/15/zensur-jetzt-geht-es-telegram-an-den-kragen/\n",
      "Progress saved to extracted_data_progress.json\n",
      "Processing URL: https://philosophia-perennis.com/2018/03/05/rt-stahl/\n",
      "Progress saved to extracted_data_progress.json\n",
      "Processing URL: https://philosophia-perennis.com/2017/06/29/bundestag-netzwerkdurchsetzungsgesetz/\n",
      "Progress saved to extracted_data_progress.json\n"
     ]
    }
   ],
   "source": [
    "progress_filename = 'extracted_data_progress.json'\n",
    "\n",
    "for index, row in df_urls.iterrows():\n",
    "    url = row['URL']\n",
    "\n",
    "    if url in processed_urls_set:\n",
    "        print(f\"URL already processed, skipping: {url}\")\n",
    "        continue # Skip to the next URL\n",
    "\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    try:\n",
    "        downloaded_html = fetch_url(url)\n",
    "        if downloaded_html:\n",
    "            extracted_data = extract(downloaded_html, output_format=\"json\", with_metadata=True)\n",
    "            if extracted_data:\n",
    "                extracted_data_list.append(extracted_data)\n",
    "                # Save progress after each successful extraction\n",
    "                try:\n",
    "                    with open(progress_filename, 'w') as f:\n",
    "                        json.dump(extracted_data_list, f, indent=4)\n",
    "                    # Add the successfully processed URL to the set\n",
    "                    processed_urls_set.add(url)\n",
    "                    print(f\"Progress saved to {progress_filename}\")\n",
    "                except IOError as e:\n",
    "                    print(f\"Error saving progress to {progress_filename}: {e}\")\n",
    "            else:\n",
    "                print(f\"Could not extract data from: {url}\")\n",
    "        else:\n",
    "            print(f\"Could not download content from: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e539a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data saved to extracted_philosophia_perennis_data.csv\n",
      "\n",
      "Head of the extracted data DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"title\": \"Zensur: Jetzt geht es \\\"Telegram\\\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"title\": \"Interview mit Rechtsanwalt Dr. Stah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"title\": \"Bundestag: Bekämpfung von News, die...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {\"title\": \"Zensur: Jetzt geht es \\\"Telegram\\\" ...\n",
       "1  {\"title\": \"Interview mit Rechtsanwalt Dr. Stah...\n",
       "2  {\"title\": \"Bundestag: Bekämpfung von News, die..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info of the extracted data DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       3 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 156.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df_extracted = pd.DataFrame(extracted_data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file without the index\n",
    "output_filename = 'extracted_philosophia_perennis_data.csv'\n",
    "df_extracted.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Extracted data saved to {output_filename}\")\n",
    "\n",
    "# Print the head and info of the created DataFrame\n",
    "print(\"\\nHead of the extracted data DataFrame:\")\n",
    "display(df_extracted.head())\n",
    "\n",
    "print(\"\\nInfo of the extracted data DataFrame:\")\n",
    "df_extracted.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
